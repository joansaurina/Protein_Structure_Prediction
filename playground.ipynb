{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Diffusion Models for Monomeric Protein Structure Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pymol\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metrics(file_path):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Define columns of interest\n",
    "    columns = [\n",
    "        'pLDDT_Fold', 'PAE_Fold', 'pTM_Fold', 'RMSD_Fold',\n",
    "        'pLDDT_Protenix', 'PAE_Protenix', 'pTM_Protenix',\n",
    "        'RMSD_Protenix', 'PAE_Chai', 'pTM_Chai', 'RMSD_Chai'\n",
    "    ]\n",
    "\n",
    "    # Ensure numeric values are correctly formatted\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace(',', '.', regex=False)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Define metric groups\n",
    "    metrics_groups = {\n",
    "        'PAE': [col for col in columns if 'PAE' in col],\n",
    "        'pLDDT': [col for col in columns if 'pLDDT' in col],\n",
    "        'pTM': [col for col in columns if 'pTM' in col],\n",
    "        'RMSD': [col for col in columns if 'RMSD' in col]\n",
    "    }\n",
    "\n",
    "    # Calculate mean metrics by group\n",
    "    mean_metrics = {\n",
    "        'Metric Group': [],\n",
    "        'Model': [],\n",
    "        'Mean Value': []\n",
    "    }\n",
    "\n",
    "    for metric_name, metric_columns in metrics_groups.items():\n",
    "        for col in metric_columns:\n",
    "            model_name = col.split('_')[-1]  # Extract model name\n",
    "            mean_value = df[col].mean()\n",
    "            mean_metrics['Metric Group'].append(metric_name)\n",
    "            mean_metrics['Model'].append(model_name)\n",
    "            mean_metrics['Mean Value'].append(mean_value)\n",
    "\n",
    "    # Create DataFrame for means\n",
    "    df_mean_metrics = pd.DataFrame(mean_metrics)\n",
    "\n",
    "    # Create pivot table for better visualization\n",
    "    df_mean_pivot = df_mean_metrics.pivot(index='Metric Group', columns='Model', values='Mean Value')\n",
    "    df_mean_pivot.index.name = None\n",
    "    df_mean_pivot.columns.name = None\n",
    "\n",
    "    # Display the table\n",
    "    print('\\n')\n",
    "    print(\"Mean by metric group:\")\n",
    "    print('\\n')\n",
    "    print(df_mean_pivot)\n",
    "\n",
    "    # Calculate mean metrics by difficulty level\n",
    "    if 'Difficulty' in df.columns:\n",
    "        mean_metrics_by_difficulty = {\n",
    "            'Difficulty': [],\n",
    "            'Metric Group': [],\n",
    "            'Model': [],\n",
    "            'Mean Value': []\n",
    "        }\n",
    "\n",
    "        for difficulty, group in df.groupby('Difficulty'):\n",
    "            for metric_name, metric_columns in metrics_groups.items():\n",
    "                for col in metric_columns:\n",
    "                    model_name = col.split('_')[-1]\n",
    "                    mean_value = group[col].mean()\n",
    "                    mean_metrics_by_difficulty['Difficulty'].append(difficulty)\n",
    "                    mean_metrics_by_difficulty['Metric Group'].append(metric_name)\n",
    "                    mean_metrics_by_difficulty['Model'].append(model_name)\n",
    "                    mean_metrics_by_difficulty['Mean Value'].append(mean_value)\n",
    "\n",
    "        # Create DataFrame for means by difficulty\n",
    "        df_mean_metrics_by_difficulty = pd.DataFrame(mean_metrics_by_difficulty)\n",
    "\n",
    "        # Create pivot table for better visualization\n",
    "        df_mean_pivot_by_difficulty = df_mean_metrics_by_difficulty.pivot(\n",
    "            index=['Difficulty', 'Metric Group'],\n",
    "            columns='Model',\n",
    "            values='Mean Value'\n",
    "        )\n",
    "        df_mean_pivot_by_difficulty.index.names = [None, None]\n",
    "        df_mean_pivot_by_difficulty.columns.name = None\n",
    "\n",
    "        # Display the table\n",
    "        print('\\n')\n",
    "        print(\"Mean by metric group and difficulty:\")\n",
    "        print('\\n')\n",
    "        print(df_mean_pivot_by_difficulty)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_full_data_file_alphafold(file_path, unique_id):\n",
    "    \"\"\"\n",
    "    Process a single JSON file ending in '_full_data.json' or '_full_data_0.json'.\n",
    "    Compute and return the mean of 'atom_plddts' and 'pae' for the specified file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract arrays\n",
    "    atom_plddts = data.get(\"atom_plddts\", [])\n",
    "    pae = data.get(\"pae\", [])\n",
    "\n",
    "    # Compute means\n",
    "    atom_plddts_mean = np.mean(atom_plddts) if atom_plddts else float('nan')\n",
    "    pae_mean = np.mean(pae) if pae else float('nan')\n",
    "\n",
    "    # Format means with comma as decimal separator\n",
    "    atom_plddts_mean_str = f\"{atom_plddts_mean:,.2f}\".replace(\",\", \"temp\").replace(\".\", \",\").replace(\"temp\", \".\")\n",
    "    pae_mean_str = f\"{pae_mean:,.2f}\".replace(\",\", \"temp\").replace(\".\", \",\").replace(\"temp\", \".\")\n",
    "\n",
    "    return unique_id, atom_plddts_mean_str, pae_mean_str\n",
    "\n",
    "def process_full_data_file_protenix(file_path, unique_id):\n",
    "    \"\"\"\n",
    "    Process a single JSON file ending in '_full_data.json' or '_full_data_0.json'.\n",
    "    Compute and return the mean of 'atom_plddts' and 'pae' for the specified file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract arrays\n",
    "    atom_plddts = data.get(\"atom_plddt\", [])\n",
    "    pae = data.get(\"token_pair_pae\", [])\n",
    "\n",
    "    # Compute means\n",
    "    atom_plddts_mean = np.mean(atom_plddts) if atom_plddts else float('nan')\n",
    "    pae_mean = np.mean(pae) if pae else float('nan')\n",
    "\n",
    "    # Format means with comma as decimal separator\n",
    "    atom_plddts_mean_str = f\"{atom_plddts_mean:,.2f}\".replace(\",\", \"temp\").replace(\".\", \",\").replace(\"temp\", \".\")\n",
    "    pae_mean_str = f\"{pae_mean:,.2f}\".replace(\",\", \"temp\").replace(\".\", \",\").replace(\"temp\", \".\")\n",
    "\n",
    "    return unique_id, atom_plddts_mean_str, pae_mean_str\n",
    "\n",
    "def process_pae_npy(file_path):\n",
    "    \"\"\"\n",
    "    Process a .npy file to extract the mean PAE value from a 2D array.\n",
    "    \"\"\"\n",
    "    # Load the .npy file\n",
    "    pae_data = np.load(file_path)\n",
    "    \n",
    "    # Flatten the 2D array and compute the mean\n",
    "    pae_mean = np.mean(pae_data) if pae_data.size > 0 else float('nan')\n",
    "    \n",
    "    # Format the mean with comma as the decimal separator\n",
    "    pae_mean_str = f\"{pae_mean:,.2f}\".replace(\",\", \"temp\").replace(\".\", \",\").replace(\"temp\", \".\")\n",
    "    \n",
    "    return pae_mean_str\n",
    "\n",
    "def process_chai_json(file_path):\n",
    "    \"\"\"\n",
    "    Process a JSON file for Chai to extract relevant metrics like PAE.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    ptm = data.get(\"ptm\", float('nan'))\n",
    "    ptm_str = f\"{ptm:,.2f}\".replace(\",\", \"temp\").replace(\".\", \",\").replace(\"temp\", \".\")\n",
    "    return ptm_str\n",
    "\n",
    "def process_summary_confidences_file(file_path):\n",
    "    \"\"\"\n",
    "    Process a single JSON file ending in '_summary.json' or '_summary_confidences_0.json'.\n",
    "    Extract and return the 'ptm' and 'iptm' scores for the specified file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract 'ptm' and 'iptm' scores\n",
    "    ptm_score = data.get(\"ptm\", float('nan'))\n",
    "\n",
    "    # Format scores with comma as decimal separator\n",
    "    ptm_score_str = f\"{ptm_score:,.2f}\".replace(\",\", \"temp\").replace(\".\", \",\").replace(\"temp\", \".\") if ptm_score is not None else \"N/A\"\n",
    "\n",
    "    return ptm_score_str\n",
    "\n",
    "def convert_cif_to_pdb(cif_file, pdb_file):\n",
    "    \"\"\"\n",
    "    Converts a CIF file to PDB format using OpenBabel.\n",
    "    \"\"\"\n",
    "    subprocess.run(['obabel', cif_file, '-O', pdb_file], check=True)\n",
    "    #print(f\"Converted CIF to PDB: {cif_file} -> {pdb_file}\")\n",
    "\n",
    "def pymol_rmsd(structure1, structure2, graph=False):\n",
    "    pymol.finish_launching(['pymol', '-c'])\n",
    "\n",
    "    structure1 = os.path.abspath(structure1)\n",
    "    structure2 = os.path.abspath(structure2)\n",
    "\n",
    "    # Get the file extensions\n",
    "    ext1 = structure1.split('.')[-1].lower()\n",
    "    ext2 = structure2.split('.')[-1].lower()\n",
    "\n",
    "    # Check if the file types are different and convert CIF to PDB if needed\n",
    "    if ext1 != ext2:\n",
    "        if ext1 == 'cif' and ext2 == 'pdb':\n",
    "            #print(f\"File formats are different. Converting {structure1} from CIF to PDB.\")\n",
    "            structure1_pdb = os.path.splitext(structure1)[0] + '.pdb'\n",
    "            convert_cif_to_pdb(structure1, structure1_pdb)\n",
    "            structure1 = structure1_pdb  # Update the structure1 path to the converted PDB\n",
    "        elif ext1 == 'pdb' and ext2 == 'cif':\n",
    "            #print(f\"File formats are different. Converting {structure2} from CIF to PDB.\")\n",
    "            structure2_pdb = os.path.splitext(structure2)[0] + '.pdb'\n",
    "            convert_cif_to_pdb(structure2, structure2_pdb)\n",
    "            structure2 = structure2_pdb  # Update the structure2 path to the converted PDB\n",
    "\n",
    "    unique_name1 = f\"structure1_{uuid.uuid4().hex[:6]}\"\n",
    "    unique_name2 = f\"structure2_{uuid.uuid4().hex[:6]}\"\n",
    "    \n",
    "    # Load the structures\n",
    "    pymol.cmd.load(structure1, unique_name1)\n",
    "    pymol.cmd.load(structure2, unique_name2)\n",
    "\n",
    "    # Superimpose the structures\n",
    "    rmsd_value = pymol.cmd.super(unique_name1, unique_name2)\n",
    "\n",
    "    if graph:\n",
    "        current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        output_path = os.path.join(current_dir, \"aligned_structures.png\")\n",
    "\n",
    "        pymol.cmd.show(\"cartoon\", unique_name1)\n",
    "        pymol.cmd.show(\"cartoon\", unique_name2)\n",
    "        pymol.cmd.color(\"red\", unique_name1)\n",
    "        pymol.cmd.color(\"blue\", unique_name2)\n",
    "        pymol.cmd.zoom(unique_name1)\n",
    "        pymol.cmd.zoom(unique_name2)\n",
    "\n",
    "        pymol.cmd.png(output_path, width=800, height=600, dpi=300)\n",
    "        #print(f\"Image saved as: {output_path}\")\n",
    "\n",
    "    pymol.cmd.delete(\"all\")\n",
    "\n",
    "    pymol.finish_launching(['pymol', '-c'])\n",
    "\n",
    "    return rmsd_value[0]\n",
    "\n",
    "def process_files(folder_path):\n",
    "    \"\"\"\n",
    "    Main function that goes through all JSON files in the specified folder,\n",
    "    calling appropriate processing functions for each file type.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            csv_filename = os.path.join(folder_path, filename)\n",
    "            break\n",
    "    df = pd.read_csv(csv_filename)\n",
    "\n",
    "    # Iterate over files in the folder\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            # Check if the file matches the pattern for fold_{unique_id}_full_data_0.json\n",
    "            if filename.endswith(\"_full_data_0.json\") and filename.startswith(\"fold_\"):\n",
    "                # Extract the unique identifier\n",
    "                match = re.search(r\"fold_([\\d_]+)_full_data_0\\.json\", filename)\n",
    "                if match:\n",
    "                    unique_id = match.group(1)\n",
    "                    file_path = os.path.join(root, filename)\n",
    "                    unique_id, atom_plddts_mean_str, pae_mean_str = process_full_data_file_alphafold(file_path, unique_id)\n",
    "\n",
    "                    # Look for the corresponding fold_{unique_id}_summary_confidences_0.json file\n",
    "                    summary_filename = f\"fold_{unique_id}_summary_confidences_0.json\"\n",
    "                    summary_file_path = os.path.join(root, summary_filename)\n",
    "                    ptm_score_str = process_summary_confidences_file(summary_file_path)\n",
    "\n",
    "                    adjusted_unique_id = unique_id[:10].replace(\"_\", \"-\") + unique_id[10:]\n",
    "\n",
    "                    # RMSD calculation\n",
    "                    #structure1 = os.path.join(root, f\"fold_{unique_id}_model_0.cif\")\n",
    "                    \n",
    "                    #structure2 = os.path.join(root, f\"{adjusted_unique_id}_target.pdb\")\n",
    "\n",
    "                    #rmsd_value = pymol_rmsd(structure1, structure2, graph=False)\n",
    "                \n",
    "                    print(f\"File ID: {unique_id}\")\n",
    "                    print(\"Alphafold 3 Metrics\")\n",
    "                    print(f\"Mean of pLDDT Score: {atom_plddts_mean_str}\")\n",
    "                    print(f\"Mean of PAE Score: {pae_mean_str}\")\n",
    "                    print(f\"PTM Score: {ptm_score_str}\")\n",
    "                    #print(f\"RMSD Value: {rmsd_value:.4f} Å\\n\")\n",
    "\n",
    "                    row_index = df[df['Target'] == unique_id].index\n",
    "                    if not row_index.empty:\n",
    "                        row = row_index[0]\n",
    "                        df.loc[row, 'pLDDT_Fold'] = atom_plddts_mean_str\n",
    "                        df.loc[row, 'PAE_Fold'] = pae_mean_str\n",
    "                        df.loc[row, 'pTM_Fold'] = ptm_score_str\n",
    "                        #df.loc[row, 'RMSD_Fold'] = f\"{rmsd_value:.4f}\"\n",
    "\n",
    "            # Check if the file matches the pattern for protenix_{unique_id}_full_data.json\n",
    "            elif filename.endswith(\"_full_data_sample_0.json\") and filename.startswith(\"Photenix_\"):\n",
    "                # Extract the unique identifier\n",
    "                match = re.search(r\"Photenix_([\\d_]+)_full_data_sample_0\\.json\", filename)\n",
    "                if match:\n",
    "                    unique_id = match.group(1)\n",
    "                    file_path = os.path.join(root, filename)\n",
    "                    unique_id, atom_plddts_mean_str, pae_mean_str = process_full_data_file_protenix(file_path, unique_id)\n",
    "\n",
    "                    # Look for the corresponding protenix_{unique_id}_summary.json file\n",
    "                    summary_filename = f\"Photenix_{unique_id}_summary_confidence_sample_0.json\"\n",
    "                    summary_file_path = os.path.join(root, summary_filename)\n",
    "                    ptm_score_str = process_summary_confidences_file(summary_file_path)\n",
    "\n",
    "                    adjusted_unique_id = unique_id[:10].replace(\"_\", \"-\") + unique_id[10:]\n",
    "\n",
    "                    # RMSD calculation\n",
    "                    #structure1 = os.path.join(root, f\"Photenix_{unique_id}_sample_0.cif\")\n",
    "                    #structure2 = os.path.join(root, f\"{adjusted_unique_id}_target.pdb\")\n",
    "\n",
    "                    #rmsd_value = pymol_rmsd(structure1, structure2, graph=False)\n",
    "\n",
    "                    print(f\"File ID: {unique_id}\")\n",
    "                    print(\"Protenix Metrics\")\n",
    "                    print(f\"Mean of pLDDT Score: {atom_plddts_mean_str}\")\n",
    "                    print(f\"Mean of PAE Score: {pae_mean_str}\")\n",
    "                    print(f\"PTM Score: {ptm_score_str}\")\n",
    "                    #print(f\"RMSD Value: {rmsd_value:.4f} Å\\n\")\n",
    "\n",
    "                    row_index = df[df['Target'] == unique_id].index\n",
    "                    if not row_index.empty:\n",
    "                        row = row_index[0]\n",
    "                        df.loc[row, 'pLDDT_Protenix'] = float(atom_plddts_mean_str.replace(',', '.'))\n",
    "                        df.loc[row, 'PAE_Protenix'] = float(pae_mean_str.replace(',', '.'))\n",
    "                        df.loc[row, 'pTM_Protenix'] = float(ptm_score_str.replace(',', '.'))\n",
    "                        #df.loc[row, 'RMSD_Protenix'] = f\"{rmsd_value:.4f}\"\n",
    "\n",
    "            if filename.endswith(\"_0.npy\") and filename.startswith(\"Chai_\"):\n",
    "                unique_id = re.search(r\"Chai_([\\d_]+)_rank_0\\.npy\", filename).group(1)\n",
    "\n",
    "                if unique_id == \"2024_07_06_00000149\":\n",
    "                    new_unique_id = f\"{unique_id}_2\"\n",
    "                else:\n",
    "                    new_unique_id = f\"{unique_id}_1\"\n",
    "\n",
    "                file_path = os.path.join(root, filename)\n",
    "                pae_mean_str = process_pae_npy(file_path)\n",
    "\n",
    "                json_file_path = os.path.join(root, f\"Chai_{unique_id}_rank_0.json\")\n",
    "\n",
    "                ptm_str = process_chai_json(json_file_path)\n",
    "\n",
    "                adjusted_unique_id = new_unique_id[:10].replace(\"_\", \"-\") + new_unique_id[10:]\n",
    "\n",
    "                # RMSD calculation\n",
    "                structure1 = os.path.join(root, f\"Chai_{unique_id}_sample_0.cif\")\n",
    "                #structure2 = os.path.join(root, f\"{adjusted_unique_id}_target.pdb\")\n",
    "\n",
    "                #rmsd_value = pymol_rmsd(structure1, structure2, graph=False)\n",
    "\n",
    "                print(f\"File ID: {unique_id}\")\n",
    "                print(\"Chai Metrics\")\n",
    "                print(f\"Mean PAE Score: {pae_mean_str}\")\n",
    "                print(f\"Aggregate Score: {ptm_str}\")\n",
    "                #print(f\"RMSD Value: {rmsd_value:.4f} Å\\n\")\n",
    "\n",
    "                row_index = df[df['Target'] == new_unique_id].index\n",
    "                if not row_index.empty:\n",
    "                    row = row_index[0]\n",
    "                    df.loc[row, 'PAE_Chai'] = pae_mean_str\n",
    "                    df.loc[row, 'pTM_Chai'] = ptm_str\n",
    "                    #df.loc[row, 'RMSD_Chai'] = f\"{rmsd_value:.4f}\"\n",
    "        \n",
    "    df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_files('data')\n",
    "process_metrics('data/data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
